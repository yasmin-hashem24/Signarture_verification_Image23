{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from commonfunctions import *\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "\n",
    "from skimage import  transform,exposure\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.morphology import binary_erosion, binary_dilation, binary_closing,skeletonize, thin\n",
    "from skimage.measure import find_contours\n",
    "from skimage.draw import rectangle\n",
    "from PIL import Image\n",
    "\n",
    "#ML \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage import feature, color\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading and gray scale conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readingAndConvert(img_path: str) -> np.ndarray:\n",
    "    image = io.imread(img_path)\n",
    "    if len(image.shape) == 2:   \n",
    "        return image\n",
    "    else:\n",
    "        gray_img = color.rgb2gray(image)\n",
    "        return gray_img\n",
    "\n",
    "forged_imgs = os.listdir('dataset/forged')\n",
    "forged_img_imgs=[]\n",
    "for i in range(len(forged_imgs)):\n",
    "    forged_img_imgs.append(readingAndConvert('dataset/forged/'+forged_imgs[i]))\n",
    "\n",
    "\n",
    "real_imgs = os.listdir('dataset/real')\n",
    "real_img_imgs=[]\n",
    "for i in range(len(real_imgs)):\n",
    "    real_img_imgs.append(readingAndConvert('dataset/real/'+real_imgs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image resizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image,target_size=(256, 256)):\n",
    "    \n",
    "    resized_image = transform.resize(image, target_size)\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "resized_forged=[]\n",
    "for i in range(len(forged_img_imgs)):\n",
    "    resized_forged.append(resize_image(forged_img_imgs[i]))\n",
    "    \n",
    "\n",
    "resized_real=[]\n",
    "for i in range(len(real_img_imgs)):\n",
    "    resized_real.append(resize_image(real_img_imgs[i]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gamma correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the gamma value (e.g., 0.5 for darkening, 2.0 for brightening)\n",
    "gamma_value = 1.5\n",
    "\n",
    "gamma_corrected_forged=[]\n",
    "for i in range(len(resized_forged)):\n",
    "    gamma_corrected_forged.append(exposure.adjust_gamma(resized_forged[i], gamma=gamma_value))\n",
    " \n",
    "gamma_corrected_real=[]\n",
    "for i in range(len(resized_real)):\n",
    "    gamma_corrected_real.append(exposure.adjust_gamma(resized_real[i], gamma=gamma_value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pattern averging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO WE EVEN NEED THAT\n",
    "\n",
    "#how to use that we have no array of images for the same image A3AAAAAAAAAAAAAAAAAAAAAAA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surf Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sift Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Approach \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Load and preprocess the dataset\u001b[39;00m\n\u001b[0;32m     54\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/path/to/your/dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 55\u001b[0m features, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and testing sets\u001b[39;00m\n\u001b[0;32m     58\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(features, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[58], line 21\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(data_dir)\u001b[0m\n\u001b[0;32m     19\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperson_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_genuine_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m signature_image \u001b[38;5;241m=\u001b[39m load_and_preprocess_image(image_path)\n\u001b[1;32m---> 21\u001b[0m hog_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_hog_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignature_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(hog_features)\n\u001b[0;32m     23\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Label 0 for genuine signatures\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[58], line 4\u001b[0m, in \u001b[0;36mextract_hog_features\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_hog_features\u001b[39m(image):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Convert the image to grayscale\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     gray_image \u001b[38;5;241m=\u001b[39m \u001b[43mcolor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrgb2gray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Extract HOG features\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     hog_features \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m.\u001b[39mhog(gray_image)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\skimage\\_shared\\utils.py:326\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\skimage\\color\\colorconv.py:875\u001b[0m, in \u001b[0;36mrgb2gray\u001b[1;34m(rgb, channel_axis)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;129m@channel_as_last_axis\u001b[39m(multichannel_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrgb2gray\u001b[39m(rgb, \u001b[38;5;241m*\u001b[39m, channel_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    836\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute luminance of an RGB image.\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \n\u001b[0;32m    838\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m    >>> img_gray = rgb2gray(img)\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 875\u001b[0m     rgb \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_colorarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m     coeffs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.2125\u001b[39m, \u001b[38;5;241m0.7154\u001b[39m, \u001b[38;5;241m0.0721\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mrgb\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    877\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rgb \u001b[38;5;241m@\u001b[39m coeffs\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\skimage\\color\\colorconv.py:137\u001b[0m, in \u001b[0;36m_prepare_colorarray\u001b[1;34m(arr, force_copy, channel_axis)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the shape of the array and convert it to\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03mfloating point representation.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    135\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchannel_axis\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    138\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe input array must have size 3 along `channel_axis`, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    139\u001b[0m            \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to extract HOG features from an image\n",
    "def extract_hog_features(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = color.rgb2gray(image)\n",
    "    # Extract HOG features\n",
    "    hog_features = feature.hog(gray_image)\n",
    "    return hog_features\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def load_dataset(data_dir):\n",
    "    # Initialize lists to store features and labels\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Loop through each person in the dataset\n",
    "    for person_id in range(1, 31):\n",
    "        # Load genuine signatures\n",
    "        for i in range(1, 6):\n",
    "            image_path = os.path.join(data_dir, f'person_{person_id}_genuine_{i}.jpg')\n",
    "            signature_image = load_and_preprocess_image(image_path)\n",
    "            hog_features = extract_hog_features(signature_image)\n",
    "            features.append(hog_features)\n",
    "            labels.append(0)  # Label 0 for genuine signatures\n",
    "\n",
    "        # Load forged signatures   #mtdeesh kolo real then kolo forged\n",
    "        for i in range(1, 6):\n",
    "            image_path = os.path.join(data_dir, f'person_{person_id}_forged_{i}.jpg')\n",
    "            signature_image = load_and_preprocess_image(image_path)\n",
    "            hog_features = extract_hog_features(signature_image)\n",
    "            features.append(hog_features)\n",
    "            labels.append(1)  # Label 1 for forged signatures\n",
    "\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Load the image (you might need to adjust this based on your actual data loading)\n",
    "    image = load_image(image_path)\n",
    "    # Preprocess the image (resize, normalize, etc.)\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    return preprocessed_image\n",
    "\n",
    "# Placeholder functions for image loading and preprocessing\n",
    "def load_image(image_path):\n",
    "    # Implement image loading based on your dataset\n",
    "    # (e.g., using OpenCV, PIL, or any other library)\n",
    "    pass\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Implement image preprocessing steps (e.g., resizing, normalizing)\n",
    "    pass\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data_dir = \"/path/to/your/dataset\"\n",
    "features, labels = load_dataset(data_dir)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Support Vector Machine (SVM) model\n",
    "svm_model = SVC(kernel='linear', C=1.0)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a new signature image\n",
    "new_signature_path = \"/path/to/new/signature.jpg\"\n",
    "new_signature_image = load_and_preprocess_image(new_signature_path)\n",
    "new_signature_features = extract_hog_features(new_signature_image)\n",
    "\n",
    "# Reshape the features array to be compatible with the model's input shape\n",
    "new_signature_features = new_signature_features.reshape(1, -1)\n",
    "\n",
    "# Use the trained model to make predictions\n",
    "prediction = svm_model.predict(new_signature_features)\n",
    "\n",
    "# Display the prediction\n",
    "if prediction == 0:\n",
    "    print(\"The signature is predicted to be genuine.\")\n",
    "else:\n",
    "    print(\"The signature is predicted to be forged.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
